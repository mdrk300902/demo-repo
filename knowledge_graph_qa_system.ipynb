{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPN45TbRy+R6UyJLCfWrbu6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdrk300902/demo-repo/blob/main/knowledge_graph_qa_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Hybrid Knowledge Retrieval and Question Answering System using LangChain, Neo4j, and Flan-T5**\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This project demonstrates how to build a knowledge retrieval and question answering system using LangChain, Neo4j graph database, and a large language model (LLM) based on Google's Flan-T5.\n",
        "\n",
        "The system loads content from Wikipedia related to \"Large language model,\" processes and splits the documents, and transforms them into a knowledge graph stored in Neo4j. Using vector embeddings and full-text search on Neo4j, the system retrieves relevant information in response to user questions.\n",
        "\n",
        "The retrieved content is combined and summarized by the Flan-T5 language model to produce concise, natural language answers, enabling structured and unstructured knowledge integration.\n",
        "\n",
        "This project showcases:\n",
        "\n",
        "- Integration of language models with graph databases for enhanced information retrieval,\n",
        "- Techniques for document chunking and embedding to handle large data efficiently,\n",
        "- The use of advanced prompt engineering and token length management for large model inference.\n",
        "\n",
        "It serves as a foundation for building scalable, hybrid retrieval-augmented generation (RAG) applications that combine structured data querying with deep language understanding.\n",
        "\n",
        "  \n",
        "   \n",
        "       \n",
        "\n",
        "Import Necessary libraries\n"
      ],
      "metadata": {
        "id": "UPIFnjRrYtRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-neo4j langchain-experimental transformers sentencepiece neo4j wikipedia tiktoken json-repair langchain-huggingface sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu8MkhGhIyx0",
        "outputId": "f0e57ee6-d2ed-4cb6-ec0e-3add4bcf8953"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.11.0 requires tenacity<9.0.0,>=8.0.0, but you have tenacity 9.1.2 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Next, import all necessary libraries and modules used throughout the project:\n",
        "\n"
      ],
      "metadata": {
        "id": "u_ExMaq4ZPUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_neo4j import Neo4jGraph\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "import warnings\n",
        "import logging\n",
        "from transformers.utils import logging as transformers_logging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahFAWDlNZFld",
        "outputId": "55829e42-52cb-4ef1-f208-f4ab9335ca44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppress warnings"
      ],
      "metadata": {
        "id": "lb4XkYkOZXcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")                        # Suppress all Python warnings\n",
        "logging.getLogger().setLevel(logging.ERROR)              # Suppress logging ERROR level\n",
        "transformers_logging.set_verbosity_error()               # Suppress HuggingFace Transformers warnings"
      ],
      "metadata": {
        "id": "kGCO-XbyOY4k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These environment variables are later used by the Neo4j client in your code to establish a connection.\n",
        "\n"
      ],
      "metadata": {
        "id": "H8nVdCPBZvbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"NEO4J_URI\"] = \"neo4j_uri\"\n",
        "os.environ[\"NEO4J_USERNAME\"] = \"neo4j_username\"\n",
        "os.environ[\"NEO4J_PASSWORD\"] = \"neo4j_password\""
      ],
      "metadata": {
        "id": "YzKVeGlLQV_s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Setup HuggingFace pipeline on GPU device 0\n"
      ],
      "metadata": {
        "id": "wD02B2J-aKX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-large\",\n",
        "    tokenizer=\"google/flan-t5-large\",\n",
        "    device=0,  #\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "0rMdEsp7QWCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load and split Wikipedia docs into chunks of 256 tokens with 50 overlap"
      ],
      "metadata": {
        "id": "RHnN1zBAaRlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_loader = WikipediaLoader(query=\"Large language model\")\n",
        "docs = wiki_loader.load()\n",
        "text_splitter = TokenTextSplitter(chunk_size=256, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(docs[:3])"
      ],
      "metadata": {
        "id": "ot_78WdzQWE3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Neo4j graph client"
      ],
      "metadata": {
        "id": "x5oTtIEsaV74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = Neo4jGraph(\n",
        "    url=os.environ[\"NEO4J_URI\"],\n",
        "    username=os.environ[\"NEO4J_USERNAME\"],\n",
        "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Iq4j9EzrQWHq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create knowledge graph from documents"
      ],
      "metadata": {
        "id": "-A7W6R5QabNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_transformer = LLMGraphTransformer(llm=llm)\n",
        "graph_documents = llm_transformer.convert_to_graph_documents(documents)"
      ],
      "metadata": {
        "id": "wYgW1htPQWJw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add graph documents to Neo4j"
      ],
      "metadata": {
        "id": "cTJnjRq6aeXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.add_graph_documents(graph_documents, include_source=True)"
      ],
      "metadata": {
        "id": "n5CBVUZmQWMa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Neo4jVector for hybrid similarity search"
      ],
      "metadata": {
        "id": "XLpr7eJCahH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "vector_index = Neo4jVector.from_existing_graph(\n",
        "    embedding=embedding_model,\n",
        "    search_type=\"hybrid\",\n",
        "    node_label=\"Document\",\n",
        "    text_node_properties=[\"text\"],\n",
        "    embedding_node_property=\"embedding\",\n",
        ")\n",
        "\n",
        "class Entities(BaseModel):\n",
        "    names: list[str] = Field(..., description=\"Extracted entities\")\n",
        "\n",
        "def simple_entity_extractor(text: str) -> list[str]:\n",
        "    return [\"Large language model\", \"AI\"]\n",
        "\n",
        "def structured_retriever(question: str) -> str:\n",
        "    entities = simple_entity_extractor(question)\n",
        "    result = \"\"\n",
        "    for entity in entities:\n",
        "        query = f\"CALL db.index.fulltext.queryNodes('keyword', '{entity}', {{limit:5}}) YIELD node RETURN node\"\n",
        "        res = graph.query(query)\n",
        "        result += f\"Data for entity '{entity}': {res}\\n\"\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "dbyMV65kQWPI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strict prompt builder enforcing max total tokens limit (512) as it is as much as the free colab T4 GPU supports"
      ],
      "metadata": {
        "id": "dp5cVsFJan9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_limited_prompt(context, question, tokenizer, max_total_tokens=512):\n",
        "    prompt_template = \"\"\"Answer concisely based on context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "    full_prompt = prompt_template.format(context=context, question=question)\n",
        "    tokens = tokenizer.tokenize(full_prompt)\n",
        "    if len(tokens) > max_total_tokens:\n",
        "        tokens = tokens[:max_total_tokens]\n",
        "    truncated_prompt = tokenizer.convert_tokens_to_string(tokens)\n",
        "    return truncated_prompt\n",
        "\n",
        "def answer_question(question: str) -> str:\n",
        "    context = retriever(question)\n",
        "    truncated_prompt = build_limited_prompt(context, question, pipe.tokenizer, max_total_tokens=512)\n",
        "    return llm(truncated_prompt)\n",
        "\n",
        "def retriever(question: str) -> str:\n",
        "    structured = structured_retriever(question)\n",
        "    unstructured_docs = vector_index.similarity_search(question)\n",
        "\n",
        "    context = \"\\n\".join(doc.page_content for doc in unstructured_docs)\n",
        "    return f\"Structured:\\n{structured}\\n\\nUnstructured:\\n{context}\"\n",
        "\n",
        "def answer_question(question: str) -> str:\n",
        "    context = retriever(question)\n",
        "    truncated_prompt = build_limited_prompt(context, question, pipe.tokenizer, max_total_tokens=512)\n",
        "    return llm(truncated_prompt)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Q: What is a large language model?\")\n",
        "    print(\"A:\", answer_question(\"What is a large language model?\"))\n",
        "\n",
        "    print(\"\\nQ: What is the name of the first large language model?\")\n",
        "    print(\"A:\", answer_question(\"What is the name of the first large language model?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n1PZSL0QWRs",
        "outputId": "afc19d3e-0c2f-468f-db24-81fa50df9799"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is a large language model?\n",
            "A: ['node': 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.nnThis page lists notable large language modelsnnFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec  1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.nnn== See also ==nList of chatbotsnList of language model benchmarksnn]\n",
            "\n",
            "Q: What is the name of the first large language model?\n",
            "A: ['node': 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.nnThis page lists notable large language modelsnnFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec  1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.nnn== See also ==nList of chatbotsnList of language model benchmarksnn]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "This project successfully combines Neo4j graph database technology with large language models to build an efficient knowledge retrieval and question answering system.\n",
        "\n",
        "By chunking, embedding, and indexing Wikipedia content, the system handles both structured graph queries and unstructured text search. The Flan-T5 model then synthesizes concise answers within token limits.\n",
        "\n",
        "This architecture demonstrates a powerful hybrid approach to scalable, retrieval-augmented language understanding which can be extended to many domains.\n"
      ],
      "metadata": {
        "id": "pPmvaOaRa7NH"
      }
    }
  ]
}